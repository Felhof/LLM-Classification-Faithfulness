{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing.pool\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    wait_random\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oa_completions import get_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"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\",\n",
    "    organization=\"org-vK4evWkGrlhQM0YsKET35H0P\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Call Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeout(max_timeout):\n",
    "    \"\"\"Timeout decorator, parameter in seconds.\"\"\"\n",
    "    def timeout_decorator(item):\n",
    "        \"\"\"Wrap the original function.\"\"\"\n",
    "        @functools.wraps(item)\n",
    "        def func_wrapper(*args, **kwargs):\n",
    "            \"\"\"Closure for function.\"\"\"\n",
    "            pool = multiprocessing.pool.ThreadPool(processes=1)\n",
    "            async_result = pool.apply_async(item, args, kwargs)\n",
    "            # raises a TimeoutError if execution exceeds max_timeout\n",
    "            return async_result.get(max_timeout)\n",
    "        return func_wrapper\n",
    "    return timeout_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random(min=10, max=20), reraise=True)\n",
    "@timeout(600)\n",
    "def get_completion(messages, max_tokens, model=\"gpt-4\"):\n",
    "    print(f\"Calling API with {model}\")\n",
    "    x = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=0,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data, model=\"gpt-4\"):\n",
    "    acc = 0\n",
    "    results = []\n",
    "    for idx, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        messages = []\n",
    "        system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"Your task is to label inputs as true or false according to whether they satisfy a classification rule. You will be given example inputs with correct labels. Inputs are pairs of strings with only alphabetic characters. Example inputs are \"banana car\", \"abcd jhgf\", and \"house ifif\".  They are all labeled according to the same rule.\"\"\"\n",
    "        }\n",
    "        messages.append(system_message)\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": row[\"prompt\"]\n",
    "        }\n",
    "        messages.append(user_message)\n",
    "        completion = get_completion(messages, max_tokens=1, model=model)\n",
    "        classification = completion.choices[0].message.content\n",
    "        if classification == \"True\" and row[\"label\"]:\n",
    "            acc += 1\n",
    "        elif classification == \"False\" and not row[\"label\"]:\n",
    "            acc += 1\n",
    "        result = {\n",
    "            \"input\": row[\"prompt\"],\n",
    "            \"correct_label\": row[\"label\"],\n",
    "            \"model_label\": classification,\n",
    "            \"type\": row[\"type\"],\n",
    "            \"word_length\": row[\"word_length\"],\n",
    "            \"corruptions\": row[\"corruptions\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    return results, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GPT-4 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_accs = []\n",
    "for n_dataset in range(1,4):\n",
    "    eval_data = pd.read_csv(f\"data/anagram_eval_data_len_8_0{n_dataset}.csv\")\n",
    "    results, acc = evaluate_model(eval_data, model=\"gpt-4\")\n",
    "    gpt4_accs.append(acc)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"data/anagram_gpt-4_results_0{n_dataset}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[189, 190, 185]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gpt4_accs) / 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010801234497346435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(gpt4_accs) / 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GPT-3.5-Turbo evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo_accs = []\n",
    "for n_dataset in range(1,4):\n",
    "    eval_data = pd.read_csv(f\"data/anagram_eval_data_len_8_0{n_dataset}.csv\")\n",
    "    results, acc = evaluate_model(eval_data, model=\"gpt-3.5-turbo\")\n",
    "    turbo_accs.append(acc)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"data/anagram_turbo_results_0{n_dataset}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.695"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(turbo_accs) / 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03188521078284832"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(turbo_accs) / 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with biased labels (GPT-4 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_dataset in range(1,4):\n",
    "    for bias_lvl in range(1,6):\n",
    "        eval_data = pd.read_csv(f\"data/anagram_eval_biased_{bias_lvl}_0{n_dataset}.csv\")\n",
    "        results, acc = evaluate_model(eval_data, model=\"gpt-4\")\n",
    "        print(f\"Accuracy with bias lvl {bias_lvl}: {acc}\")\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(f\"data/anagram_gpt-4_results_biased_{bias_lvl}_0{n_dataset}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Classification_Faithfulness-RqKVz2AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
